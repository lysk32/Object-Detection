{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision with detr-resnet-50 \n",
    "## Computer eyes vs human eyes?\n",
    "\n",
    "Hello everyone,\n",
    "\n",
    "This Jupyter notebook provides a step-by-step guide on how to use an open-source Object Detection (OD) model from Hugging Face to count cars in a video. The notebook leverages the `transformers` and `opencv-python` libraries to handle the tasks of processing video input and producing a labeled output video with a real-time counter. Additionally, `matplotlib` is used to display images for visualization purposes.\n",
    "\n",
    "This script is inspired by various computer vision tutorials and adapted to demonstrate practical applications of machine learning in video analysis.\n",
    "\n",
    "Recommendations:\n",
    "* `transformers` (Library for state-of-the-art machine learning models)\n",
    "* `opencv-python` (Library for computer vision tasks)\n",
    "* `Python 3.10` (click on top right corner to change version)\n",
    "* `matplotlib 3.4.3` (Library for plotting graphs, but in this case it is used to display images)\n",
    "* `torch` (Deep learning library, required for running the Hugging Face models)\n",
    "\n",
    "## Video Sources\n",
    "\n",
    "For this tutorial, we'll be using free stock videos from Pexels. You can use your own videos or download these examples:\n",
    "\n",
    "- People_walking: https://www.pexels.com/video/black-and-white-video-of-people-853889/ (works ok)\n",
    "- Dogs in snow: https://www.pexels.com/video/dogs-enjoying-the-snow-4157418/ (works meh/bad)\n",
    "- Dogs playing: https://www.pexels.com/video/dogs-playing-854179/ (works well!)\n",
    "- Cars: https://www.pexels.com/video/cars-on-highway-854671/ (works well!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Guide\n",
    "\n",
    "# 1. First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.10/site-packages (10.4.0)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venv/lib/python3.10/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./venv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pillow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the pre-trained object detection model using the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "Model and processor loaded: facebook/detr-resnet-50\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model_name = \"facebook/detr-resnet-50\"\n",
    "processor = DetrImageProcessor.from_pretrained(model_name, revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(model_name, revision=\"no_timm\").to(device)\n",
    "print(f\"Model and processor loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define some helper functions for processing frames and drawing bounding boxes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect_objects\n",
    "\n",
    "## Inputs:\n",
    "- `frame`: Single frame from the video (in BGR format)\n",
    "- `processor`: Image processor for the model\n",
    "- `model`: Computer vision model used for object detection\n",
    "- `threshold`: Confidence threshold for object detection (default: 0.7)\n",
    "\n",
    "## Imports:\n",
    "- `PIL.Image`: For image processing\n",
    "- `cv2`: OpenCV library for computer vision tasks\n",
    "\n",
    "## Logic:\n",
    "1. Prepare the image:\n",
    "   - Convert the frame from BGR to RGB color space\n",
    "   - Create a PIL Image object from the RGB frame\n",
    "\n",
    "2. Process the image:\n",
    "   - Use the `processor` to prepare the image for the model\n",
    "   - Move the processed inputs to the appropriate device (e.g., CPU or GPU)\n",
    "\n",
    "3. Perform object detection:\n",
    "   - Run the model on the processed inputs\n",
    "   - Get the output from the model\n",
    "\n",
    "4. Post-process the results:\n",
    "   - Create a tensor with the original image size\n",
    "   - Use the processor's post-processing method to refine the detection results\n",
    "   - Apply the confidence threshold to filter detections\n",
    "\n",
    "## Output:\n",
    "- Returns `results`: Processed detection results containing:\n",
    "  - Bounding boxes\n",
    "  - Labels\n",
    "  - Confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "def detect_objects(frame, processor, model, threshold=0.7):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.tensor([pil_image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw_boxes_and_count \n",
    "\n",
    "## Inputs:\n",
    "- `frame`: Single frame of the video\n",
    "- `results`: JSONL with all the object's:\n",
    "  - Box coordinates (where the object is in frame)\n",
    "  - Label (what the object is)\n",
    "  - Scores (how certain the model is the object is labeled correctly)\n",
    "- `model`: Computer vision model used to detect objects (`detr-resnet-50` in our case)\n",
    "- `thing`: Object you want to count\n",
    "\n",
    "## Logic: \n",
    "\n",
    "For every object that the model detected in the frame (packaged nicely in `results`):\n",
    "1. Check if the `label` of the object is `thing`\n",
    "2. If yes:\n",
    "   - Add one to the frame's count\n",
    "   - Draw the box around the object\n",
    "   - Add the `label` to the box\n",
    "3. After checking all the objects detected in the frame:\n",
    "   - Put the total `thing` count on the frame\n",
    "\n",
    "## Outputs:\n",
    "- `frame`: edited version of frame\n",
    "- `count`: how many things there are in frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_and_count(frame, results, model, thing):\n",
    "    count = 0\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()] == thing:\n",
    "            count += 1\n",
    "            box = [int(i) for i in box.tolist()]\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (225, 0, 0), 2)\n",
    "            label_text = f\"{thing}: {score.item():.2f}\"\n",
    "            cv2.putText(frame, label_text, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)\n",
    "    \n",
    "    cv2.putText(frame, f\"{thing} count: {count}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)    \n",
    "    return frame, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_video\n",
    "\n",
    "## Inputs:\n",
    "- `video_path`: Path to the input video file\n",
    "- `output_path`: Path where the processed video will be saved\n",
    "- `thing`: Object to detect and count in the video\n",
    "- `model`: Computer vision model used for object detection\n",
    "- `processor`: Image processor for the model\n",
    "\n",
    "## Logic:\n",
    "1. Load the video:\n",
    "   - Open the video file\n",
    "   - Get video properties (width, height, fps)\n",
    "   - Set up video writer for output\n",
    "\n",
    "2. Initialize variables:\n",
    "   - `frame_count`: Counter for processed frames\n",
    "   - `total_num_frames`: Total number of frames in the video\n",
    "   - `total_count`: Total count of detected objects across all frames\n",
    "\n",
    "3. Process each frame:\n",
    "   - Read a frame from the video\n",
    "   - If frame is empty, end processing\n",
    "   - Increment `frame_count`\n",
    "   - Display progress (current frame / total frames)\n",
    "   - Detect objects in the frame using `detect_objects` function\n",
    "   - Draw boxes and count objects using `draw_boxes_and_count` function\n",
    "   - Add the frame's object count to `total_count`\n",
    "   - Write the processed frame to the output video\n",
    "\n",
    "4. Clean up:\n",
    "   - Release video capture and writer\n",
    "   - Close all OpenCV windows\n",
    "\n",
    "5. Print results:\n",
    "   - Confirmation of video processing completion\n",
    "   - Total number of objects detected across all frames\n",
    "   - Average number of objects per frame\n",
    "\n",
    "## Output:\n",
    "- Processed video file saved to `output_path`\n",
    "- Returns the last processed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_path, thing, model, processor):\n",
    "    \n",
    "    print('Loading video...')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print('Video loaded successfully.')\n",
    "    \n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"\\rProcessing frame {frame_count}/{total_num_frames}\", end='', flush=True)\n",
    "        \n",
    "        results = detect_objects(frame, processor, model)\n",
    "        frame_with_boxes, count = draw_boxes_and_count(frame, results, model, thing)\n",
    "        \n",
    "        total_count += count\n",
    "        \n",
    "        out.write(frame_with_boxes)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f\"Video processing complete. Output saved to {output_path}\")\n",
    "    print(f\"Total number of {thing} detected across all frames: {total_count}\")\n",
    "    print(f\"Average number of {thing} per frame: {total_count / frame_count:.2f}\")\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setting the video path and processing video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video...\n",
      "Video loaded successfully.\n",
      "Processing frame 1501/1501Video processing complete. Output saved to sample videos/cars.mp4output.mp4\n",
      "Total number of car detected across all frames: 8109\n",
      "Average number of car per frame: 5.40\n"
     ]
    }
   ],
   "source": [
    "video_path = 'sample videos/cars.mp4'  # Replace with your input video path\n",
    "output_path = 'output_'+video_path  # Replace with your desired output video path\n",
    "thing = \"car\"  # Replace with the object you want to detect\n",
    "\n",
    "process_video(video_path, output_path, thing, model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ta da!\n",
    "\n",
    "Done! This script will process your input video, detect and count `thing` in each frame, and produce an output video with bounding boxes around detected `thing` and a `thing` count in the bottom right corner.\n",
    "\n",
    "Feel free to experiment with different videos or adjust the detection threshold to see how it affects the results. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
